{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget: TorchSharp-cpu\"\n",
    "\n",
    "open TorchSharp\n",
    "open type TorchSharp.torch\n",
    "open type TorchSharp.TensorExtensionMethods\n",
    "open type TorchSharp.torch.distributions\n",
    "\n",
    "open Microsoft.DotNet.Interactive.Formatting\n",
    "Formatter.SetPreferredMimeTypeFor(typeof<torch.Tensor>, \"text/plain\")\n",
    "Formatter.Register<torch.Tensor>(fun (x:torch.Tensor) -> x.ToString(true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with a Learning Rate Scheduler\n",
    "\n",
    "In Tutorial 6, we saw how the optimizers took an argument called the 'learning rate,' but didn't spend much time on it except to say that it could have a great impact on how quickly training would converge toward a solution. In fact, you can choose the learning rate (LR) so poorly, that the training doesn't converge at all.\n",
    "\n",
    "If the LR is too small, training will go very slowly, wasting compute resources. If it is too large, training could result in numeric overflow, or NaNs. Either way, you're in trouble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further complicate matters, it turns out that the learning rate shouldn't necessarily be constant. Training can go much better if the learning rate starts out relatively large and gets smaller as you get closer to the end.\n",
    "\n",
    "There's a solution for this, called a Learning Rate Scheduler. An LRS instance has access to the internal state of the optimizer, and can modify the LR as it goes along. There are several algorithms for scheduling, but TorchSharp only implements the two most conceptually simple: StepLR and ExponentialLR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before demonstrating, let's have a model and a baseline training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    }
   },
   "outputs": [],
   "source": [
    "type Trivial() as this = \n",
    "    inherit nn.Module(\"Trivial\")\n",
    "\n",
    "    let lin1 = nn.Linear(1000L, 100L)\n",
    "    let lin2 = nn.Linear(100L, 10L)\n",
    "\n",
    "    do\n",
    "        this.RegisterComponents()\n",
    "\n",
    "    override _.forward(input) = \n",
    "    \n",
    "        use x = lin1.forward(input)\n",
    "        use y = nn.functional.relu(x)\n",
    "        lin2.forward(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    }
   },
   "outputs": [],
   "source": [
    "let learning_rate = 0.01\n",
    "let model = Trivial()\n",
    "\n",
    "let dataBatch = rand(32,1000)  // Our pretend input data\n",
    "let resultBatch = rand(32,10)  // Our pretend ground truth.\n",
    "\n",
    "let loss x y = nn.functional.mse_loss().Invoke(x,y)\n",
    "\n",
    "let optimizer = torch.optim.SGD(model.parameters(), learning_rate)\n",
    "\n",
    "for epoch = 1 to 100 do\n",
    "    // Compute the loss\n",
    "    let pred = model.forward(dataBatch)\n",
    "    let output = loss pred resultBatch\n",
    "\n",
    "    // Clear the gradients before doing the back-propagation\n",
    "    model.zero_grad()\n",
    "\n",
    "    // Do back-progatation, which computes all the gradients.\n",
    "    output.backward()\n",
    "\n",
    "    optimizer.step() |> ignore\n",
    "\n",
    "let pred = model.forward(dataBatch)\n",
    "(loss pred resultBatch).item<single>()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran this, the loss was down to 0.068 after 3 seconds. (It took longer the first time around.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StepLR\n",
    "\n",
    "StepLR uses subtraction to adjust the learning rate every so often. The difference it makes to the training loop is that you wrap the optimizer, and then call `step` on the scheduler instead of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    }
   },
   "outputs": [],
   "source": [
    "let learning_rate = 0.01\n",
    "let model = Trivial()\n",
    "\n",
    "let dataBatch = rand(32,1000)  // Our pretend input data\n",
    "let resultBatch = rand(32,10)  // Our pretend ground truth.\n",
    "\n",
    "let loss x y = nn.functional.mse_loss().Invoke(x,y)\n",
    "\n",
    "let optimizer = torch.optim.SGD(model.parameters(), learning_rate)\n",
    "let scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1u, 0.99)    // Pass in 'verbose=true' if you want to see how the LR changes over time.\n",
    "\n",
    "for epoch = 1 to 100 do\n",
    "    // Compute the loss\n",
    "    let pred = model.forward(dataBatch)\n",
    "    let output = loss pred resultBatch\n",
    "\n",
    "    // Clear the gradients before doing the back-propagation\n",
    "    model.zero_grad()\n",
    "\n",
    "    // Do back-progatation, which computes all the gradients.\n",
    "    output.backward()\n",
    "\n",
    "    scheduler.step() |> ignore\n",
    "\n",
    "let pred = model.forward(dataBatch)\n",
    "(loss pred resultBatch).item<single>()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    }
   },
   "source": [
    "Well, that was underwhelming. The loss (in my case) went up just a little bit, essentially a rounding error. For this trivial model, using a scheduler isn't going to make much of a difference, and it may not make much of a difference even for complex models. It's very hard to know until you try it, but now you know how to try it out.\n",
    "\n",
    "In the future, TorchSharp will add more of the LR schedulers that are available for PyTorch, as well as allow them to be combined."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (F#)",
   "language": "F#",
   "name": ".net-fsharp"
  },
  "language_info": {
   "name": "F#"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
